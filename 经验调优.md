# Hadoop-
经验总结

## 大数据数据仓库实战项目-电商数仓教程V3.0最新版

### 1. 项目经验之 HDFS 存储多目录
若 HDFS 存储空间紧张，需要对 DataNode 进行磁盘扩展
  1. 在 DataNode 节点增加磁盘并进行挂载。 

  2. 在 hdfs-site.xml 文件中配置多目录，注意新挂载磁盘的访问权限问题。 
  ```
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///${hadoop.tmp.dir}/dfs/data1,file:///hd2/dfs/ data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
  </property>
  ```


### 2. 项目经验之 集群数据均衡
#### 1. 增加磁盘后，保证集群每个机器间数据均衡 
1. 开启数据均衡命令：
  ```
  bin/start-balancer.sh –threshold 10 
  ```
对于参数 10，代表的是集群中各个节点的磁盘空间利用率相差不超过 10%，可根据实际情况进行调整。 

2. 停止数据均衡命令：
  ```
  bin/stop-balancer.sh
  ```
 
#### 2. 集群机器每个磁盘间数据均衡
1. 生成均衡计划
  ```
  hdfs diskbalancer -plan hadoop103
  ````

2. 执行均衡计划
  ```
  hdfs diskbalancer -execute hadoop103.plan.json
  ```

3. 查看当前均衡计划的执行情况
  ```
  hdfs diskbalancer -query hadoop103
  ```

4. 取消均衡计划
  ```
  hdfs diskbalancer -cancel hadoop103.plan.json
  ```


### 3. 项目经验之 支持LZO压缩配置
将编译好后的 hadoop-lzo-0.4.20.jar 放入 hadoop-3.1.2/share/hadoop/common/

修改core-site.xml 增加配置支持 LZO 压缩
  ```
  <property>
    <name>io.compression.codecs</name>
    <value> org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec </value>
  </property>
  ```


### 4. 项目经验之 LZO 创建索引
1. 创建 LZO 文件的索引，LZO 压缩文件的可切片特性依赖于其索引，故我们需要手动为 LZO 压缩文件创建索引。若无索引，则 LZO 文件的切片只有一个
2. 测试
   将 bigtable.lzo（150M）上传到集群的根目录
    ```
    hadoop fs -put bigtable.lzo /input
    ```
   执行 wordcount 程序
    ```
    hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output1
    ```
    ![lzo_split_1](https://github.com/caocong192/Hadoop-/blob/main/pics/lzo_split_1.jpg)
    
   对上传的 LZO 文件建索引
    ```
    hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo
    ```
   再次执行 WordCount 程序
    ```
    hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output2
    ```
    ![lzo_split_2](https://github.com/caocong192/Hadoop-/blob/main/pics/lzo_split_2.jpg)


### 5. 项目经验之 基准测试

1. 测试 HDFS 写性能
测试内容：向 HDFS 集群写 3(CPU-1) 个 128M 的文件
  ```
  hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 3 -fileSize 128MB
  ```
  ```
  2020-10-14 20:29:46,588 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:             Date & time: Wed Oct 14 20:29:46 CST 2020
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:         Number of files: 3
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:  Total MBytes processed: 384
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:       Throughput mb/sec: 37.64
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:  Average IO rate mb/sec: 38.11
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:   IO rate std deviation: 4.35
  2020-10-14 20:29:46,674 INFO fs.TestDFSIO:      Test exec time sec: 51.95
  ```
2. 测试 HDFS 读性能
测试内容：读取 HDFS 集群 3(CPU-1) 个 128M 的文件
  ```
  hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 3 -fileSize 128MB
  ```
  ```
  2020-10-14 20:32:48,460 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
  2020-10-14 20:32:48,461 INFO fs.TestDFSIO:             Date & time: Wed Oct 14 20:32:48 CST 2020
  2020-10-14 20:32:48,463 INFO fs.TestDFSIO:         Number of files: 3
  2020-10-14 20:32:48,463 INFO fs.TestDFSIO:  Total MBytes processed: 384
  2020-10-14 20:32:48,463 INFO fs.TestDFSIO:       Throughput mb/sec: 226.55
  2020-10-14 20:32:48,463 INFO fs.TestDFSIO:  Average IO rate mb/sec: 241.69
  2020-10-14 20:32:48,463 INFO fs.TestDFSIO:   IO rate std deviation: 62.22
  2020-10-14 20:32:48,463 INFO fs.TestDFSIO:      Test exec time sec: 41.12
  ```

3. 使用 Sort 程序评测 MapReduce
4. 使用 RandomWriter 来产生随机数，每个节点运行 10 个 Map 任务，每个 Map 产 生大约 1G 大小的二进制随机数
   ```
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
   ```

5. 执行 Sort 程序
   ```
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
   ```
  
6. 验证数据是否真正排好序了
   ```
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
   ```
    
    
### 6. 项目经验之 Hadoop 参数调优
1. HDFS 参数调优 hdfs-site.xml
建议值: int(20*math.log(集群个数))
   ```
    <property>
      <name>dfs.namenode.handler.count</name>
      <value>10</value> 
    </property>
   ```

2. YARN 参数调优 yarn-site.xml
  单个任务可以申请的最大内存 大小，和 Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率
    ```
    yarn.nodemanager.resource.memory-mb
    ```
  表示该节点上 YARN 可使用的物理内存总量，默认是 8192（MB）， 注意，如果你的节点 内存资源不够 8GB，则需要调减小这个值，而 YARN 不会智能的探测节点的物理内存总量
    ```
    yarn.scheduler.maximum-allocation-mb
    ```
  单个任务可申请的最多物理内存量，默认是 8192（MB）
  
  单个任务默认8G、 单个MapTask 默认1G、 单个ReduceTask默认1G
  128M数据 = 1G 内存
